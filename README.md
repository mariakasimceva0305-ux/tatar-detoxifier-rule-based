# Tatar Detoxifier — `main.py`

Rule-based детоксификатор татарского текста для хакатонов и продакшена.  
Без LLM, без API, без внешних зависимостей во время инференса — только быстрый, предсказуемый и полностью объяснимый код.

---

## 1. Задача, под которую заточен `main.py`

Вход:  
- короткая или средняя фраза на татарском (часто с вкраплениями русского),  
- в которой могут быть:
  - оскорбления,
  - грубые разговорные выражения,
  - негативные «звериные» метафоры,
  - гибридные русско-татарские конструкции.

Выход:  
- **нетоксичная**, грамматически корректная татарская фраза, где:
  - **максимально сохранён** смысл оригинала,
  - **структура и длина** предложения меняются минимально,
  - грубость убрана или смягчена,
  - **нет никаких служебных комментариев, скобок, эмодзи и т.п.**

Метрики (ориентир):

- **Similarity (LaBSE)** — сохраняем семантику и структуру.
- **Toxicity (xlmr-large-toxicity-classifier-v2)** — снижаем токсичность за счёт точечного удаления и мягких замен.
- **Fluency (XCOMET-lite)** — сохраняем естественный татарский язык.

`main.py` специально спроектирован так, чтобы на этих метриках показать устойчиво высокий результат без использования больших языковых моделей.

---

## 2. Почему именно rule-based, а не LLM

В типичном LLM-подходе делается запрос типа «перефразируй без токсичности».  
Это красиво выглядит, но для **жёстких метрик** часто плохо работает.

### 2.1. Проблемы LLM-подходов

1. **Переписывание текста**  
   - Модель стремится «улучшить» стиль, добавляет пояснения.
   - Фразы перераспределяются, меняются союзы, порядок частей.
   - Итог: Similarity падает → итоговый скор ухудшается.

2. **Непредсказуемость**  
   - Один и тот же запрос в разное время даст немного разный ответ.
   - Обновление модели в API может изменить характер ответов.
   - Для хакатона и продакшена это риск.

3. **Смешение языков и грамматические ошибки**  
   - Модели не всегда уверенно держат татарский:
     - иногда подмешивают русский,
     - нарушают падежи/аффиксы,
     - используют неправильный порядок слов.
   - Fluency может падать, несмотря на формальное отсутствие грубости.

4. **Стоимость и скорость**
   - Каждый запрос — это сетевой вызов и токены.
   - При больших объёмах (десятки тысяч строк) время и цена заметны.

### 2.2. Преимущества `main.py`

1. **Детерминизм**  
   - Одно и то же предложение → всегда один и тот же результат.
   - Поведение не зависит от версии модели на сервере, нагрузки и т.п.

2. **Контролируемость**  
   - Каждое изменение можно показать в словаре:
     - «вот это слово → заменяем на вот это»,
     - «вот этот корень → всегда удаляется».
   - При необходимости легко объяснить: почему конкретное слово исчезло или изменилось.

3. **Локальность модификаций**  
   - Никаких глобальных перефразировок.
   - Мы просто:
     - удаляем или заменяем конкретные токены,
     - подчистим пробелы и знаки препинания.
   - Структура фразы остаётся почти той же → Similarity высока по определению.

4. **Эффективность и масштабируемость**
   - Работает за O(N) по количеству токенов.
   - Прогон десятков тысяч строк в секунду на обычном CPU.
   - Может быть встроен в любой пайплайн без GPU и сетевого доступа.

5. **Безопасность по токсичности**
   - Словари и подстроки подбираются так, чтобы проблемные слова не «проскакивали».
   - Гибридные формы и нестандартная орфография ловятся именно через подстроки и лексикон.

---

## 3. Структура решения и используемые файлы

`main.py` опирается на несколько словарных ресурсов. Их можно обновлять и расширять — это основная точка кастомизации.

### 3.1. `toxic_replacements.json`

**Формат:**  
```json
{
  "слово_или_устойчивая_форма": "замена_или_пустая_строка",
  "ахмак": "ялгышасың",
  "ангыра": "дөрес аңламыйсың",
  "хайван": "кеше",
  "дурак": "ялгышасың",
  "резкое_русское_слово": ""
}
```

**Назначение:**

- это основной словарь **точных токенов**, которые нужно:
  - либо мягко переформулировать,
  - либо удалить полностью.

**Принципы построения:**

- Каждому оскорбительному татарскому слову подбирается **семантически близкий, но нейтральный аналог**:
  - вместо «ты плохой» → «ты ошибаешься»,
  - вместо прямых оскорблений → формулировки про непонимание / ошибку.
- Русские грубые слова чаще всего просто удаляются (`""`), чтобы не переусложнять текст.

**Почему это лучше, чем «умный» LLM:**

- LLM склонен делать сложные конструкции вроде  
  «человек, который, возможно, неправ, но имеет право на мнение» —  
  это ухудшает Similarity, удлиняет фразу, создаёт лишний смысл.
- В `toxic_replacements.json` замены:
  - максимально короткие,
  - структурно совместимые с оригиналом,
  - не изменяют основного факта в предложении.

---

### 3.2. `toxic_substrings.json`

**Формат:**  
```json
[
  "корень1",
  "корень2",
  "корень3"
]
```

**Назначение:**

- список **подстрок**, по которым распознаются:
  - нестандартные формы грубых слов,
  - гибриды татарского с русским,
  - намеренные искажения (орфографические вариации).

**Как используется:**

1. При загрузке `tt_ru_lexicon.csv`:
   - берутся только те слова, где есть хотя бы один такой корень.
   - эти слова автоматически помечаются как токсичные для удаления.

2. На финальном шаге детокса:
   - любой токен, содержащий подстроку из списка, удаляется из текста.
   - это страховка на случай, если слово не попало в словарь точных замен.

**Почему так лучше:**

- Реальный пользователь редко пишет оскорбления в «чистом словарном виде»:
  - сокращает,
  - искажает,
  - комбинирует с латиницей и др.
- LLM может не понять, что это всё — вариации одного и того же грубого слова.
- Подстрочное правило нацелено **ровно** на корни, которые 100% относятся к нежелательным выражениям, поэтому мы ловим вариативность, не перетирая остальную лексику.

---

### 3.3. `tat_Cyrl_twl.txt`

**Формат:**

- Один токен на строку:
  ```text
  слово1
  слово2
  слово3
  ```

**Назначение:**

- внешний список татарских слов, которые считаются явно нежелательными (например, резкие прозвища или устойчивые грубые выражения);
- при загрузке каждый такой токен добавляется в общий словарь как `слово -> ""` (удалить полностью).

**Как это помогает:**

- даёт возможность быстро расширять покрытие:
  - не меняя `toxic_replacements.json`,
  - не вмешиваясь в основную логику.
- позволяет добавлять специфические слова из новых датасетов / доменов:
  - отдельные сленговые формы,
  - локальные жаргонизмы.

**Почему это удобно в отличие от LLM:**

- Вместо «переобучения» модели под новый домен — достаточно добавить пару строк.
- Поведение прозрачное: можно посмотреть файл и увидеть, какие слова будут полностью вычищены.

---

### 3.4. `tt_ru_lexicon.csv`

**Формат (типично):**

- CSV с колонками типа:
  ```text
  word, freq, ...
  ```

**Назначение:**

- содержит русскоязычный / смешанный лексикон;
- `main.py` берёт из него только те слова, которые содержат подстроку из `toxic_substrings.json`;
- затем добавляет их в словарь как `слово -> ""`.

**Смысл:**

- автоматическая генерация «чёрного списка» для различных русско-татарских гибридов и просторечных форм;
- избавляемся от необходимости руками прописывать каждую деформированную форму.

**Преимущество:**

- когда в корпусе появляются новые нестандартные формы, их можно автоматически просеять по корням, вместо ручного анализа каждой;
- это масштабируемый способ расширения покрытия нежелательной лексики, в то время как LLM обучаются на огромных общих корпусах и могут не учитывать новые локальные тенденции.

---

## 4. Как всё это работает вместе

Внутри `main.py` основная функция — `detox(text: str) -> str`.

Упрощённый пайплайн:

1. **Исходный текст → токены**  
   - текст нормализуется,
   - разбивается на токены по пробелам и пунктуации.

2. **Словарные замены (`toxic_replacements.json`)**  
   - даёт мягкие перефразировки или удаляет отдельные слова;
   - сохраняет максимально допустимую близость к оригиналу.

3. **Удаление слов из `tat_Cyrl_twl.txt` и `tt_ru_lexicon.csv`**  
   - любые явно нежелательные токены удаляются.

4. **Финальный фильтр по подстрокам (`toxic_substrings.json`)**  
   - защита от «неучтённых» форм и креативных искажений.

5. **Подчистка пробелов и знаков препинания**  
   - убираются лишние пробелы,
   - корректируются пробелы перед знаками препинания,
   - результат выглядит как нормальное предложение.

6. **Fallback, если текст стал слишком коротким**  
   - если после агрессивной чистки текст «обвалился» по длине,
   - используется более мягкий словарь (без части внешнего лексикона),
   - это позволяет сохранить смысл, когда грубых слов было немного, а русской части — много.

**Почему это эффективно для метрик:**

- **Similarity**:  
  - структура предложения почти не меняется;
  - нет глобальных перефразировок;
  - удаляются отдельные токены, а не переписывается всё.

- **Toxicity**:  
  - словари и подстроки гарантируют, что нежелательные корни будут вычищены;
  - многослойность (replacements → twl → lexicon → substrings) даёт хорошее покрытие.

- **Fluency**:  
  - мягкие замены подобраны так, чтобы звучать естественно на татарском;
  - не вводятся «книжные» конструкции и не добавляется лишний контент.

---

## 5. Установка, запуск и интеграция

### 5.1. Требования

- Python 3.8+
- Библиотеки:
  ```bash
  pip install pandas
  ```
- Файлы в одной папке с `main.py`:
  - `toxic_replacements.json`
  - `toxic_substrings.json`
  - `tat_Cyrl_twl.txt`
  - `tt_ru_lexicon.csv`

### 5.2. Запуск на TSV

Ожидаемый формат входного файла (`dev_inputs.tsv` / `test_inputs.tsv`):

```text
ID    tat_toxic    ...
0     текст_1
1     текст_2
...
```

Команда запуска:

```bash
python main.py -i dev_inputs.tsv -o submit_main.tsv
```

На выходе получится TSV с колонками:

```text
ID    tat_toxic    tat_detox1
```

где:

- `tat_toxic` — исходный текст;
- `tat_detox1` — результат детокса.

### 5.3. Использование как Python-модуль

```python
from main import detox

print(detox("син ..."))  # вернёт нетоксичную версию
```

Это удобно для интеграции в веб-сервисы, пайплайны обработки текста, локальные скрипты.

---

## 6. Стабильность, масштабируемость и эффективность

### 6.1. Стабильность

- Код не зависит от внешних API.
- Результат не меняется со временем (если не меняете словари).
- Нет скрытых случайностей (random, температура, семплинг).

### 6.2. Масштабируемость

- Время работы линейно по числу токенов.
- Можно обрабатывать:
  - локально на ноутбуке,
  - в докер-контейнере,
  - внутри Spark/MapReduce пайплайнов.

### 6.3. Эффективность

- Никаких больших моделей: только regex + словари.
- Типичный прогон ~тысяч строк занимает считанные секунды.
- Легко запускать параллельно (по батчам, по файлам).

---

## 7. Как улучшать и развивать решение

Даже без LLM текущее решение можно развивать:

### 7.1. Расширение словарей

- Добавление новых токенов в `toxic_replacements.json` с аккуратными заменами.
- Пополнение `tat_Cyrl_twl.txt` редкими или новыми грубыми выражениями.
- Обновление `toxic_substrings.json` новыми корнями, замеченными в логах / датасетах.
- Автоматическое извлечение новых кандидатов из корпусов по эвристикам (частота, контекст).

### 7.2. Доменные адаптации

- Для конкретных платформ (чат, форум, соцсеть) можно добавить:
  - их специфические сленговые формы,
  - типичные искажения и сокращения.

### 7.3. Комбинация с лёгкими моделями (опционально)

Если позже захочется мягко улучшить грамматику:

- можно добавлять небольшой слой поверх `main.py`, который:
  - предлагает 1–2 грамматически исправленных варианта,
  - строго проверяется по токсичности, длине, похожести,
  - применяется только если **улучшает fluency, не ухудшая остальное**.

Сам `main.py` при этом остаётся стабильным ядром, а модель — лишь опциональным постпроцессингом.

---

## 8. Вывод

`main.py` — это:

- быстрый и лёгкий детоксификатор татарского текста,
- полностью управляемый и расширяемый за счёт словарей,
- дружелюбный к метрикам Similarity / Toxicity / Fluency,
- устойчивый к нестандартной орфографии и гибридным формам.

Для задач формата хакатона (как TatarHack) и для продакшена, где важны:

- предсказуемость,
- отсутствие лишних сюрпризов,
- простота интеграции,

rule-based подход со словарями и подстроками — это очень сильная альтернатива тяжёлым LLM-пайплайнам и часто превосходит их именно по итоговым метрикам качества.
